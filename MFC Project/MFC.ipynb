{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa8ab8a-0a45-4c8f-b42c-26abdc3a19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfe6111-a28e-4816-b8a1-dc15fbc5af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.io.wavfile\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "import sympy\n",
    "import math\n",
    "from math import comb\n",
    "import string\n",
    "import functools as fn\n",
    "from fractions import Fraction\n",
    "import hashlib\n",
    "from mpmath import mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aedc8a-6966-406e-a0f2-046bbb4cd75e",
   "metadata": {},
   "source": [
    "#### INTRODUCTION\n",
    " \n",
    "As we now, especially for machine learning, managable and compressed datasets for research make the proccess easier to implementation, reduce storage space. So for developers, main task is to reduce database with minimal errors without losing any important or relevant data. Nowdays speech recognition or any other type of speech system is very important. \n",
    "There are many techniques for keeping signal information while reduce the dimension. Mel-frequency Cepstrum Coefficient is designed to model features of audio signal and is widely used in various fields. \n",
    " \n",
    "Description From Wikipedia:  \n",
    "\"In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\"\n",
    " \n",
    "#### Steps for implementation:\n",
    " \n",
    "For feature extraction, Mel Frequency Cepstrum Coefficient (MFCC) is capable to extract harmonics and sidebands of the spectrum version of the signal in some period of time. MFCCs are composing the MFC. These coefficients are derived form the type of cepstral representation of the audio clip. MFCCs provide a compact representation of the audio signal that captures the important features related to human hearing. These features can then be used in applications like speech recognition.\n",
    " \n",
    "The difference in MFC is that, as we know humans can hear between some determined frequencies in non-linear way. Mel scale is presenting (convert) actual frequency scale to scale in a way human percieve the sounds. \n",
    " \n",
    "Sound wave is a wave, which can be presented like series of numbers (samples). Samples \"sample rates\" are measured in Hz, which we can call Frequency. Spectrum sound is the presentation of all frequancies and amplitudes. Amplitude present how intense is the wave in that time. Power of spectrum is telling how much of different frequincies is present in a sound signal at some time. \n",
    " \n",
    "Cepstrum is helping to investigate and separate the different components of the sound signal. To get the cepstrum, you compute the logarithm of the power spectrum (to compress the range of values) and then perform a transformation called the Discrete Cosine Transform (DCT). Actually Cepstrum is just Spectrum over Spectrum.\n",
    " \n",
    "Humans are more sensitive to changes in quiet sounds than in very loud sounds. Taking the logarithm of the power spectrum helps match how humans hear loudness.\n",
    " \n",
    "If we summirize all the information, there are few steps we have to go trough with corresponding equations:\n",
    " \n",
    "1. Pre-emphasis filter: On this step, we have difference equation, which amplify the high frequencies $$y(t) = x(t) - \\alpha x(t-1)$$ where $\\alpha \\in (0.95, 0.97) $ this is the pre-emphasis coefficient, $x(t)$ is the original signal\n",
    "2. Framing: gets sliced into (overlapping) frames $$x_{frames}(n) = x(n + mM)$$ $m = 0,1,2,\\dots,[\\frac{N-L}{M}]$ - cover all possible frames, where $N$ is the signal length, $M$ is the frame step, $L$ is the length of each frame in the samples; $n$ is in range $(0, L-1)$ and cover lenght of 1 frame\n",
    "3. Windowing: Window function is applied to each frame. Used concept is Hamming window, smoothing functions. The hamming window gives accurate information of the original signal's frequency spectrum after sclicing. $$w[n] = 0.54 - 0.46cos(\\frac{2\\pi n}{N-1})$$ where $0\\leq n \\leq N-1$, $N$ is the window length. For whole signal we can compute: $$x_{w}(n) = x(n)*w(n)$$\n",
    "4. FFT and Power Spectrum: we do a Fourier transform on each frame (or more specifically a Short-Time Fourier Transform) and calculate the power spectrum. Usually $N$ is signal of each frame. The equation for FFT is $$X(k) = \\sum_{n=0}^{N-1}x_{w}(n)e^{-j\\frac{2\\pi kn}{N}}$$ the equation for computing Power Spectrum is: $$P(k) = \\frac{|X(k)^2|}{N}$$ $k = 1,\\dots K$, $K = 257$, number of discrete Fourier transformation coefficients \n",
    "5. Mel Filterbank: Uses logarithmic functions and linear transformations to mimic human auditory perception. Now we convert the power spectrum of the signal from a linear frequency scale to a Mel frequency scale. The Mel scale approximates the human ear's response more closely than the linear scale. $$m = 2595\\log_{10}\\left (1+\\frac{f}{700}\\right)$$ After we can convert again $Mel$ to $frequency$: $$f = 700\\left(10^{\\frac{m}{2595}} -1\\right)$$ \n",
    "6. Logarithm: Converts to the logarithmic scale for better dynamic range representation $$f_{mel} = log\\sum{P(k).H_m(k)}$$ where $$\\begin{align*} H_{m}\\left ({k }\\right)=\\begin{cases} \\displaystyle 0 & k < f\\left ({m-1 }\\right)\\\\ \\displaystyle \\frac {k-f\\left ({m-1 }\\right)}{f\\left ({m }\\right)-f\\left ({m-1 }\\right)} & f\\left ({m-1 }\\right)\\le k < f\\left ({m }\\right)\\\\ \\displaystyle 1 & k=f\\left ({m }\\right)\\\\ \\displaystyle \\frac {f\\left ({m+1 }\\right)-k}{f\\left ({m+1 }\\right)-f(m)} & f\\left ({m }\\right) < k\\le f\\left ({m+1 }\\right)\\\\ \\displaystyle 0 & k>f(m+1) \\end{cases}\\!\\!\\!\\! \\\\{}\\tag{2}\\end{align*}$$\n",
    "7. Discrete Cosine Transform (DCT) retaining a number of the resulting coefficients while the rest are discarded. On this step we decorrelate the log filterbank energies and pack the most significant information into the smallest number of coefficients.  $$ C_n = \\sum_{m=1}^{M}L_m\\cos\\left[\\frac{\\pi n\\left(2m-1\\right)}{2M}\\right]$$ where,\n",
    "$C_n$ - n-th Mel coefficient,\n",
    "$L_m$ is the m-th Logarithm from step 7,\n",
    "$M$ is Mel filters number\n",
    " \n",
    "#### Applications\n",
    " \n",
    "##### 1. Acoustic Analysis \n",
    "   1.1 Speech Analysis\n",
    "   1.2 Biometric Application\n",
    "   1.3. Digital Forensic\n",
    "3. Medical Applications\n",
    "   2.1 EEG Analysis\n",
    "   2.2 ECG Analysis\n",
    "   2.3 Disease Detection Application\n",
    "4. Industry Analysis\n",
    "   3.1 Gear Health Monitoring\n",
    "   3.2 Bearing Health Monitoring\n",
    "   3.3 Turbine Health Monitoring\n",
    "   3.4 Pump Health Monitoring\n",
    " \n",
    " \n",
    " \n",
    "#### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08604949-45cd-4455-b0c4-4359718080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal(signal):\n",
    "    samplerate, audio = scipy.io.wavfile.read(signal) \n",
    "    return samplerate, audio    \n",
    " \n",
    "def pre_emphasis(audio):\n",
    "    pre = np.array([(audio[i]- 0.97 * audio[i-1]) if i > 0 else audio[i] for i in np.arange(0, len(audio))]) # data in signal in pre-emphasis  \n",
    "    return pre\n",
    " \n",
    "def audio_frames(audio, sample_rate, frame_length, frame_step):\n",
    "    signal_length = len(audio)\n",
    "    frame_samples_length = frame_length * sample_rate \n",
    "    frames_step_samples_length = frame_step * sample_rate\n",
    "    num_frames = int((signal_length - frame_samples_length)/frames_step_samples_length) + 1 # +one frame left last without step\n",
    " \n",
    "    n = np.array([np.arange(0, int(frame_samples_length),dtype='int64') for i in np.arange(0, int(num_frames),dtype='int64')]) # all samples/frame for every frame\n",
    "    m_M = np.array([(np.arange(0, int(num_frames),dtype='int64'))*int(frames_step_samples_length) for i in np.arange(0, int(frame_samples_length),dtype='int64')]).T # from equation m*M for every frame, array should be transformed, this is \n",
    "    all_frames_x = n + m_M\n",
    " \n",
    "    new_audio = audio\n",
    "    #if we don't have equal length we add zeros in the end. This is because each frame overlap with frame_step\n",
    "    length_zeros = signal_length - int(num_frames * frames_step_samples_length + frame_samples_length)\n",
    "    if (length_zeros < 0):\n",
    "        new_audio = np.append(new_audio, np.zeros(abs(length_zeros)))\n",
    " \n",
    "    audio_frames = new_audio[all_frames_x] # the audio sliced on segments/frames    \n",
    "    return audio_frames\n",
    " \n",
    "def window(framed_audio, frame_length):    \n",
    "    hamm_w = np.array([0.54 - 0.46*np.cos((np.pi*2*i)/(frame_length-1)) for i in np.arange(0,frame_length-1)]) # to check N or N-1\n",
    "    audio_framed_window = hamm_w * framed_audio #hamming window (vector) multiply by each frame\n",
    "    #print(len(audio_framed_window), audio_framed_window[0])\n",
    "    return audio_framed_window\n",
    " \n",
    " \n",
    "def stft(window_audio, frame_length):\n",
    "    mag_frames = np.absolute(np.fft.rfft(window_audio, 512)) # just for check\n",
    "    audio_length = len(window_audio)\n",
    "    fft_coeff = 258\n",
    "    x_k = []\n",
    " \n",
    "    for w in np.arange(0, audio_length):     \n",
    "        sftf_frames = []\n",
    "        for c in np.arange(1, fft_coeff):  \n",
    "            stft = 0\n",
    "            for n in np.arange(0, int(frame_length)): \n",
    "                stft += window_audio[w, n] * np.exp((-2j * np.pi * c * n) / frame_length)\n",
    "            sftf_frames.append(stft)\n",
    "        x_k.append(sftf_frames)\n",
    "\n",
    "    x_k = np.array(x_k)\n",
    "    power_spec = np.abs( np.array(x_k))**2\n",
    "    print(power_spec[0], x_k[0]) \n",
    "    return x_k\n",
    " \n",
    "def power_spectrum(sftf, frame_length): \n",
    "    pc = np.array([(sftf[k]**2)/frame_length for k in np.arange(0,len(sftf))])\n",
    "    #print(type(pc))\n",
    "    return pc\n",
    " \n",
    "def h_mel_transform_filters(samplerate, numbers_filters): # convert Hz to Mel and divide on the triangularers Mel filters, usually they are 26 filters\n",
    "    mels = np.linspace(0, 2595*np.log10(1+ (samplerate/2)/700), numbers_filters) # low frequency = 0, high - samplerates/2\n",
    "    return mels\n",
    " \n",
    "def mel_frequency_convert(mels): # convert again Mel to Hz \n",
    "    frequency_mel = (700 * (10**(mels/2595) - 1))\n",
    "    return frequency_mel\n",
    " \n",
    "def mel_filterbanks(frequency_mel, power_spectrum):\n",
    " \n",
    "    return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd51b14-a3d1-41d8-a1e2-5a2a6e308604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nushi\\AppData\\Local\\Temp\\ipykernel_20592\\1354170873.py:2: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  samplerate, audio = scipy.io.wavfile.read(signal)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'math' has no attribute 'abs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m frame_audio \u001b[38;5;241m=\u001b[39m audio_frames(pre, samplerate, frame_length, frame_step)\n\u001b[0;32m     10\u001b[0m window \u001b[38;5;241m=\u001b[39m window(frame_audio, frame_length \u001b[38;5;241m*\u001b[39m samplerate)\n\u001b[1;32m---> 11\u001b[0m stft \u001b[38;5;241m=\u001b[39m stft(window, frame_length \u001b[38;5;241m*\u001b[39m samplerate)\n",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(window_audio, frame_length)\u001b[0m\n\u001b[0;32m     48\u001b[0m     x_k\u001b[38;5;241m.\u001b[39mappend(sftf_frames)\n\u001b[0;32m     50\u001b[0m x_k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_k)\n\u001b[1;32m---> 51\u001b[0m power_spec \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mabs( np\u001b[38;5;241m.\u001b[39marray(x_k))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(power_spec[\u001b[38;5;241m0\u001b[39m], x_k[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_k\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'math' has no attribute 'abs'"
     ]
    }
   ],
   "source": [
    "samplerate, audio = load_signal(\"c-note.wav\")\n",
    "\n",
    "# plt.plot(audio)\n",
    "# plt.show()\n",
    "frame_length = 0.025\n",
    "frame_step = 0.01\n",
    "numbers_filters = 26\n",
    "pre = pre_emphasis(audio)\n",
    "frame_audio = audio_frames(pre, samplerate, frame_length, frame_step)\n",
    "window = window(frame_audio, frame_length * samplerate)\n",
    "stft = stft(window, frame_length * samplerate) \n",
    "#power_spectrum = (stft, frame_length*samplerate)\n",
    "#mel_transform = h_mel_transform_filters(samplerate, numbers_filters)\n",
    "#mel_frequency_convert = mel_frequency_convert(mel_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
