{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa8ab8a-0a45-4c8f-b42c-26abdc3a19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccfe6111-a28e-4816-b8a1-dc15fbc5af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.io.wavfile\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "import sympy\n",
    "import math\n",
    "from math import comb\n",
    "import string\n",
    "import functools as fn\n",
    "from fractions import Fraction\n",
    "import hashlib\n",
    "from mpmath import mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aedc8a-6966-406e-a0f2-046bbb4cd75e",
   "metadata": {},
   "source": [
    "#### INTRODUCTION\n",
    " \n",
    "As we now, especially for machine learning, managable and compressed datasets for research make the proccess easier to implementation, reduce storage space. So for developers, main task is to reduce database with minimal errors without losing any important or relevant data. Nowdays speech recognition or any other type of speech system is very important. \n",
    "There are many techniques for keeping signal information while reduce the dimension. Mel-frequency Cepstrum Coefficient is designed to model features of audio signal and is widely used in various fields. \n",
    " \n",
    "Description From Wikipedia:  \n",
    "\"In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\"\n",
    " \n",
    "#### Steps for implementation:\n",
    " \n",
    "For feature extraction, Mel Frequency Cepstrum Coefficient (MFCC) is capable to extract harmonics and sidebands of the spectrum version of the signal in some period of time. MFCCs are composing the MFC. These coefficients are derived form the type of cepstral representation of the audio clip. MFCCs provide a compact representation of the audio signal that captures the important features related to human hearing. These features can then be used in applications like speech recognition.\n",
    " \n",
    "The difference in MFC is that, as we know humans can hear between some determined frequencies in non-linear way. Mel scale is presenting (convert) actual frequency scale to scale in a way human percieve the sounds. \n",
    " \n",
    "Sound wave is a wave, which can be presented like series of numbers (samples). Samples \"sample rates\" are measured in Hz, which we can call Frequency. Spectrum sound is the presentation of all frequancies and amplitudes. Amplitude present how intense is the wave in that time. Power of spectrum is telling how much of different frequincies is present in a sound signal at some time. \n",
    " \n",
    "Cepstrum is helping to investigate and separate the different components of the sound signal. To get the cepstrum, you compute the logarithm of the power spectrum (to compress the range of values) and then perform a transformation called the Discrete Cosine Transform (DCT). Actually Cepstrum is just Spectrum over Spectrum.\n",
    " \n",
    "Humans are more sensitive to changes in quiet sounds than in very loud sounds. Taking the logarithm of the power spectrum helps match how humans hear loudness.\n",
    " \n",
    "If we summirize all the information, there are few steps we have to go trough with corresponding equations:\n",
    " \n",
    "1. Pre-emphasis filter: On this step, we have difference equation, which amplify the high frequencies $$y(t) = x(t) - \\alpha x(t-1)$$ where $\\alpha \\in (0.95, 0.97) $ this is the pre-emphasis coefficient, $x(t)$ is the original signal\n",
    "2. Framing: gets sliced into (overlapping) frames $$x_{frames}(n) = x(n + mM)$$ $m = 0,1,2,\\dots,[\\frac{N-L}{M}]$ - cover all possible frames, where $N$ is the signal length, $M$ is the frame step, $L$ is the length of each frame in the samples; $n$ is in range $(0, L-1)$ and cover lenght of 1 frame\n",
    "3. Windowing: Window function is applied to each frame. Used concept is Hamming window, smoothing functions. The hamming window gives accurate information of the original signal's frequency spectrum after sclicing. $$w[n] = 0.54 - 0.46cos(\\frac{2\\pi n}{N-1})$$ where $0\\leq n \\leq N-1$, $N$ is the window length. For whole signal we can compute: $$x_{w}(n) = x(n)*w(n)$$\n",
    "4. FFT and Power Spectrum: we do a Fourier transform on each frame (or more specifically a Short-Time Fourier Transform) and calculate the power spectrum. Usually $N$ is signal of each frame. The equation for FFT is $$X(k) = \\sum_{n=0}^{N-1}x_{w}(n)e^{-j\\frac{2\\pi kn}{N}}$$ the equation for computing Power Spectrum is: $$P(k) = \\frac{|X(k)^2|}{N}$$ $k = 1,\\dots K$, $K = 257$, number of discrete Fourier transformation coefficients \n",
    "5. Mel Filterbank: Uses logarithmic functions and linear transformations to mimic human auditory perception. Now we convert the power spectrum of the signal from a linear frequency scale to a Mel frequency scale. The Mel scale approximates the human ear's response more closely than the linear scale. $$m = 2595\\log_{10}\\left (1+\\frac{f}{700}\\right)$$ After we can convert again $Mel$ to $frequency$: $$f = 700\\left(10^{\\frac{m}{2595}} -1\\right)$$ \n",
    "6. Logarithm: Converts to the logarithmic scale for better dynamic range representation $$f_{mel} = log\\sum{P(k).H_m(k)}$$ where $$\\begin{align*} H_{m}\\left ({k }\\right)=\\begin{cases} \\displaystyle 0 & k < f\\left ({m-1 }\\right)\\\\ \\displaystyle \\frac {k-f\\left ({m-1 }\\right)}{f\\left ({m }\\right)-f\\left ({m-1 }\\right)} & f\\left ({m-1 }\\right)\\le k < f\\left ({m }\\right)\\\\ \\displaystyle 1 & k=f\\left ({m }\\right)\\\\ \\displaystyle \\frac {f\\left ({m+1 }\\right)-k}{f\\left ({m+1 }\\right)-f(m)} & f\\left ({m }\\right) < k\\le f\\left ({m+1 }\\right)\\\\ \\displaystyle 0 & k>f(m+1) \\end{cases}\\!\\!\\!\\! \\\\{}\\tag{2}\\end{align*}$$\n",
    "7. Discrete Cosine Transform (DCT) retaining a number of the resulting coefficients while the rest are discarded. On this step we decorrelate the log filterbank energies and pack the most significant information into the smallest number of coefficients.  $$ C_n = \\sum_{m=1}^{M}L_m\\cos\\left[\\frac{\\pi n\\left(2m-1\\right)}{2M}\\right]$$ where,\n",
    "$C_n$ - n-th Mel coefficient,\n",
    "$L_m$ is the m-th Logarithm from step 7,\n",
    "$M$ is Mel filters number\n",
    " \n",
    "#### Applications\n",
    " \n",
    "##### 1. Acoustic Analysis \n",
    "   1.1 Speech Analysis\n",
    "   1.2 Biometric Application\n",
    "   1.3. Digital Forensic\n",
    "3. Medical Applications\n",
    "   2.1 EEG Analysis\n",
    "   2.2 ECG Analysis\n",
    "   2.3 Disease Detection Application\n",
    "4. Industry Analysis\n",
    "   3.1 Gear Health Monitoring\n",
    "   3.2 Bearing Health Monitoring\n",
    "   3.3 Turbine Health Monitoring\n",
    "   3.4 Pump Health Monitoring\n",
    " \n",
    " \n",
    " \n",
    "#### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "08604949-45cd-4455-b0c4-4359718080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal(signal):\n",
    "    samplerate, audio = scipy.io.wavfile.read(signal) \n",
    "    return samplerate, audio    \n",
    "\n",
    "def pre_emphasis(audio):\n",
    "    pre_emphasis_signal = np.array([(audio[i]- 0.97 * audio[i-1]) if i > 0 else audio[i] for i in np.arange(0, len(audio))]) # data in signal in pre-emphasis \n",
    "    return pre_emphasis_signal\n",
    "\n",
    "def audio_frames(audio, sample_rate, frame_length, frame_step):\n",
    "    signal_length = len(audio)\n",
    "    frame_samples_length = frame_length * sample_rate \n",
    "    frames_step_samples_length = frame_step * sample_rate\n",
    "    num_frames = (signal_length - frame_samples_length)/frames_step_samples_length\n",
    "     \n",
    "    n = np.array([np.arange(0, int(frame_samples_length),dtype='int64') for i in np.arange(0, int(num_frames),dtype='int64')]) # all samples/frame for every frame\n",
    "    m_M = np.array([(np.arange(0, int(num_frames),dtype='int64'))*int(frames_step_samples_length) for i in np.arange(0, int(frame_samples_length),dtype='int64')]).T # from equation m*M for every frame, array should be transformed, this is \n",
    "    all_frames_x = n + m_M\n",
    "    audio_frames = audio[all_frames_x] # the audio sliced on segments/frames\n",
    "    return audio_frames\n",
    "\n",
    "def window(framed_audio, frame_length):\n",
    "    hamm_w = np.array([0.54 - 0.45*np.cos((np.pi*2*i)/(frame_length-1)) for i in np.arange(0,frame_length-1)]) # t check N or N-1\n",
    "    \n",
    "    audio_window = np.dot(hamm_w,framed_audio) #hamming window multiply by each frame\n",
    "    print(audio_window[1])\n",
    "    return audio_window\n",
    "\n",
    "def stft(window_audio, frame_length):\n",
    "    fur_coeff = 257\n",
    "    stft = 0\n",
    "    for c in np.arange(0, fur_coeff) :\n",
    "        for i in np.arange(0, len(window_audio)):        \n",
    "                stft += window_audio[i]*np.exp((-2j*np.pi*c*i)/len(window_audio))\n",
    "    return stft\n",
    "\n",
    "def power_spectrum(sftf, window_length): #???? which length \n",
    "    pc = np.array([abs(sftf[k]**2/window_length) for k in np.arange(0,sftf)])\n",
    "    print(pc[1])\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bcd51b14-a3d1-41d8-a1e2-5a2a6e308604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nushi\\AppData\\Local\\Temp\\ipykernel_17352\\212082122.py:2: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  samplerate, audio = scipy.io.wavfile.read(signal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2101.73487969   979.84104025]\n"
     ]
    }
   ],
   "source": [
    "samplerate, audio = load_signal(\"c-note.wav\")\n",
    "frame_length = 0.025\n",
    "frame_step = 0.01\n",
    "pre = pre_emphasis(audio)\n",
    "frame_audio = audio_frames(pre, samplerate, frame_length, frame_step)\n",
    "window = window(frame_audio, frame_length*samplerate)\n",
    "stft = stft(window, audio) # window or audio lenght \n",
    "power_spectrum = (stft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
